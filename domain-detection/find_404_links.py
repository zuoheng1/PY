import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import openpyxl
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment
from collections import deque
import re
from datetime import datetime
import os
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Link404Crawler:
    def __init__(self, domain, max_pages=100, delay=1, path_filter=None, 
                 max_workers=5, timeout=10):
        self.domain = domain
        self.base_url = self._normalize_url(domain)
        self.max_pages = max_pages
        self.delay = delay
        self.path_filter = path_filter
        self.timeout = timeout
        self.max_workers = max_workers
        
        # æ•°æ®å­˜å‚¨
        self.visited_urls = set()
        self.found_404s = []
        self.all_links = set()
        self.page_link_details = []
        self._lock = threading.Lock()
        
        # HTTPä¼šè¯é…ç½®
        self.session = self._create_session()
        
        # é™æ€èµ„æºæ–‡ä»¶æ‰©å±•ååˆ—è¡¨
        self.static_extensions = {
            '.css', '.scss', '.sass', '.less',
            '.js', '.jsx', '.ts', '.tsx', '.coffee',
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.ico', '.tiff', '.tif',
            '.woff', '.woff2', '.ttf', '.otf', '.eot',
            '.mp3', '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm', '.ogg', '.wav',
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.rtf',
            '.zip', '.rar', '.7z', '.tar', '.gz', '.bz2',
            '.xml', '.json', '.csv', '.swf', '.map'
        }
    
    def _normalize_url(self, domain):
        """æ ‡å‡†åŒ–URLæ ¼å¼"""
        if not domain.startswith(('http://', 'https://')):
            return f"https://{domain}"
        return domain
    
    def _create_session(self):
        """åˆ›å»ºHTTPä¼šè¯"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        # è®¾ç½®è¿æ¥æ± å¤§å°
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=20,
            pool_maxsize=20,
            max_retries=3
        )
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        return session
    
    def is_static_resource(self, url):
        """æ£€æŸ¥URLæ˜¯å¦ä¸ºé™æ€èµ„æºæ–‡ä»¶"""
        try:
            parsed = urlparse(url)
            path = parsed.path.lower()
            
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            if any(path.endswith(ext) for ext in self.static_extensions):
                return True
            
            # æ£€æŸ¥è·¯å¾„æ¨¡å¼
            static_patterns = [
                '/_next/static/', '/static/', '/assets/', '/public/',
                '/dist/', '/build/', '/css/', '/js/', '/images/',
                '/img/', '/fonts/', '/media/'
            ]
            
            return any(pattern in path for pattern in static_patterns)
            
        except Exception:
            return False
    
    def is_valid_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆä¸”å±äºç›®æ ‡åŸŸå"""
        try:
            parsed = urlparse(url)
            base_parsed = urlparse(self.base_url)
            
            # æ£€æŸ¥åŸŸå
            if parsed.netloc and parsed.netloc != base_parsed.netloc:
                return False
            
            # æ£€æŸ¥åè®®
            if parsed.scheme in ['mailto', 'tel', 'javascript']:
                return False
            
            # æ£€æŸ¥é”šç‚¹
            if url.startswith('#'):
                return False
            
            # æ£€æŸ¥é™æ€èµ„æº
            if self.is_static_resource(url):
                return False
            
            return True
            
        except Exception:
            return False
    
    def matches_path_filter(self, url):
        """æ£€æŸ¥URLæ˜¯å¦åŒ¹é…è·¯å¾„è¿‡æ»¤å™¨"""
        if not self.path_filter:
            return True
        
        try:
            parsed = urlparse(url)
            path = parsed.path
            
            if isinstance(self.path_filter, list):
                return any(path.startswith(filter_path) for filter_path in self.path_filter)
            else:
                # ğŸ”§ é’ˆå¯¹ /au çš„ç²¾ç¡®åŒ¹é…
                filter_path = self.path_filter
                # ç¡®ä¿è·¯å¾„ä»¥ / å¼€å¤´
                if not filter_path.startswith('/'):
                    filter_path = '/' + filter_path
                
                # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»¥ç­›é€‰æ¡ä»¶å¼€å¤´
                return path.startswith(filter_path)
                
        except Exception as e:
            logger.error(f"è·¯å¾„åŒ¹é…æ£€æŸ¥å‡ºé”™: {e}")
            return False
    
    def detect_link_position_and_classes(self, element):
        """æ£€æµ‹é“¾æ¥åœ¨é¡µé¢ä¸­çš„ä½ç½®å’Œclasså±æ€§"""
        positions = []
        classes_info = []
        current = element
        
        position_indicators = {
            'header': ['header', 'top', 'navbar', 'nav-bar', 'navigation'],
            'footer': ['footer', 'bottom', 'foot'],
            'sidebar': ['sidebar', 'side-bar', 'aside'],
            'main': ['main', 'content', 'body'],
            'menu': ['menu', 'nav', 'navigation'],
            'breadcrumb': ['breadcrumb', 'breadcrumbs'],
            'pagination': ['pagination', 'pager'],
            'search': ['search', 'search-box'],
            'social': ['social', 'share', 'follow'],
            'product': ['product', 'item', 'card'],
            'category': ['category', 'cat', 'section'],
            'banner': ['banner', 'hero', 'slider'],
            'form': ['form', 'contact', 'subscribe']
        }
        
        # æ”¶é›†å½“å‰å…ƒç´ çš„classä¿¡æ¯
        element_classes = element.get('class', [])
        if element_classes:
            classes_info.append({
                'tag': element.name,
                'classes': ' '.join(element_classes),
                'level': 'current'
            })
        
        level = 0
        while current and current.name and level < 5:  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚
            # æ£€æŸ¥HTMLæ ‡ç­¾
            if current.name in ['header', 'footer', 'nav', 'aside', 'main', 'section', 'article']:
                positions.append(f"<{current.name}>")
            
            # æ£€æŸ¥classå’Œidå±æ€§
            classes = current.get('class', [])
            element_id = current.get('id', '')
            
            # æ”¶é›†çˆ¶çº§å…ƒç´ çš„classä¿¡æ¯
            if classes and level > 0:  # ä¸é‡å¤æ”¶é›†å½“å‰å…ƒç´ 
                classes_info.append({
                    'tag': current.name,
                    'classes': ' '.join(classes),
                    'level': f'parent-{level}'
                })
            
            all_attrs = ' '.join(classes + [element_id]).lower()
            
            for position, keywords in position_indicators.items():
                if any(keyword in all_attrs for keyword in keywords):
                    positions.append(f"{position}({keywords[0]})")
                    break
            
            current = current.parent
            level += 1
        
        position_str = " > ".join(reversed(positions)) if positions else "é¡µé¢ä¸»ä½“"
        
        # ç”ŸæˆCSSé€‰æ‹©å™¨
        css_selector = self._generate_css_selector(element)
        
        # ç”ŸæˆXPath
        xpath = self._generate_xpath(element)
        
        # ç¡®å®šå¯è§†åŒ–ä½ç½®
        visual_position = self._determine_visual_position(positions, element_classes)
        
        return {
            'position': position_str,
            'classes_info': classes_info,
            'element_id': element.get('id', ''),
            'element_tag': element.name,
            'css_selector': css_selector,
            'xpath': xpath,
            'visual_position': visual_position
        }
    
    def check_url_status(self, url):
        """æ£€æŸ¥URLçš„çŠ¶æ€ç """
        try:
            # å…ˆå°è¯•HEADè¯·æ±‚
            response = self.session.head(url, timeout=self.timeout, allow_redirects=True)
            return response.status_code
        except requests.exceptions.RequestException:
            try:
                # HEADå¤±è´¥åˆ™å°è¯•GETè¯·æ±‚
                response = self.session.get(url, timeout=self.timeout, allow_redirects=True)
                return response.status_code
            except Exception as e:
                logger.warning(f"æ£€æŸ¥URLçŠ¶æ€å¤±è´¥ {url}: {e}")
                return 'ERROR'
    
    def check_urls_batch(self, urls):
        """æ‰¹é‡æ£€æŸ¥URLçŠ¶æ€"""
        results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.check_url_status, url): url for url in urls}
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    status = future.result()
                    results[url] = status
                except Exception as e:
                    logger.error(f"æ£€æŸ¥URL {url} æ—¶å‡ºé”™: {e}")
                    results[url] = 'ERROR'
        
        return results
    
    def extract_and_check_links_from_page(self, url):
        """ä»é¡µé¢æå–å¹¶æ£€æŸ¥é“¾æ¥"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æ£€æµ‹é¡µé¢: {url}")
            response = self.session.get(url, timeout=self.timeout)
            
            if response.status_code != 200:
                logger.warning(f"âš ï¸ é¡µé¢è®¿é—®å¤±è´¥: {url} (çŠ¶æ€ç : {response.status_code})")
                return set()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            links = set()
            link_positions = {}
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºé¡µé¢åŸºæœ¬ä¿¡æ¯
            logger.info(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
            
            # æå–æ‰€æœ‰é“¾æ¥
            self._extract_links_from_soup(soup, url, links, link_positions)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæå–åˆ°çš„é“¾æ¥æ•°é‡
            logger.info(f"ğŸ”— ä»é¡µé¢æå–åˆ° {len(links)} ä¸ªåŸå§‹é“¾æ¥")
            
            # è¿‡æ»¤æœ‰æ•ˆé“¾æ¥
            valid_links = set()
            for link in links:
                if self.is_valid_url(link) and not self.is_static_resource(link):
                    valid_links.add(link)
            
            logger.info(f"âœ… è¿‡æ»¤åæœ‰æ•ˆé“¾æ¥: {len(valid_links)} ä¸ª")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªé“¾æ¥ä½œä¸ºæ ·æœ¬
            if valid_links:
                logger.info(f"ğŸ“‹ é“¾æ¥æ ·æœ¬ (å‰5ä¸ª):")
                for i, link in enumerate(list(valid_links)[:5]):
                    logger.info(f"  {i+1}. {link}")
            
            page_links_status = []
            
            # æ‰¹é‡æ£€æŸ¥é“¾æ¥çŠ¶æ€
            if links:
                logger.info(f"ğŸ” æ£€æŸ¥ {len(links)} ä¸ªé“¾æ¥çš„çŠ¶æ€...")
                status_results = self.check_urls_batch(links)
                
                # å¤„ç†ç»“æœ
                for link_url in links:
                    status = status_results.get(link_url, 'ERROR')
                    position_info = link_positions.get(link_url, {
                        'position': 'æœªçŸ¥ä½ç½®', 
                        'text': '', 
                        'element_type': 'unknown',
                        'classes_info': [],
                        'element_id': '',
                        'element_tag': '',
                        'css_selector': '',
                        'xpath': '',
                        'visual_position': 'æœªçŸ¥åŒºåŸŸ'
                    })
                    
                    link_status = self._create_link_status(url, link_url, status, position_info)
                    page_links_status.append(link_status)
                    
                    # å¤„ç†404é“¾æ¥
                    if status == 404:
                        self._handle_404_link(url, link_url, position_info, link_status)
            
            # ä¿å­˜é¡µé¢è¯¦æƒ…
            self._save_page_details(url, links, page_links_status)
            
            return valid_links
            
        except Exception as e:
            logger.error(f"æå–é“¾æ¥æ—¶å‡ºé”™ {url}: {e}")
            return set()
    
    def _extract_links_from_soup(self, soup, base_url, links, link_positions):
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–é“¾æ¥"""
        # æå–aæ ‡ç­¾é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href'].strip()
            if href:
                absolute_url = urljoin(base_url, href)
                if self.is_valid_url(absolute_url):
                    clean_url = absolute_url.split('#')[0]
                    links.add(clean_url)
                    if clean_url not in link_positions:
                        position_data = self.detect_link_position_and_classes(link)
                        link_positions[clean_url] = {
                            'position': position_data['position'],
                            'text': link.get_text(strip=True)[:50],
                            'element_type': 'link',
                            'classes_info': position_data['classes_info'],
                            'element_id': position_data['element_id'],
                            'element_tag': position_data['element_tag'],
                            'css_selector': position_data['css_selector'],
                            'xpath': position_data['xpath'],
                            'visual_position': position_data['visual_position']
                        }
        
        # æå–imgæ ‡ç­¾é“¾æ¥
        for img in soup.find_all('img', src=True):
            src = img['src'].strip()
            if src:
                absolute_url = urljoin(base_url, src)
                if self.is_valid_url(absolute_url):
                    links.add(absolute_url)
                    if absolute_url not in link_positions:
                        position_data = self.detect_link_position_and_classes(img)
                        link_positions[absolute_url] = {
                            'position': position_data['position'],
                            'text': img.get('alt', '')[:50],
                            'element_type': 'image',
                            'classes_info': position_data['classes_info'],
                            'element_id': position_data['element_id'],
                            'element_tag': position_data['element_tag'],
                            'css_selector': position_data['css_selector'],
                            'xpath': position_data['xpath'],
                            'visual_position': position_data['visual_position']
                        }
    
    def _create_link_status(self, parent_url, link_url, status, position_info):
        """åˆ›å»ºé“¾æ¥çŠ¶æ€å¯¹è±¡"""
        return {
            'parent_page': parent_url,
            'link_url': link_url,
            'status_code': status,
            'check_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'matches_filter': self.matches_path_filter(link_url),
            'position': position_info['position'],
            'link_text': position_info['text'],
            'element_type': position_info['element_type'],
            'classes_info': position_info.get('classes_info', []),
            'element_id': position_info.get('element_id', ''),
            'element_tag': position_info.get('element_tag', ''),
            'css_selector': position_info.get('css_selector', ''),
            'xpath': position_info.get('xpath', ''),
            'visual_position': position_info.get('visual_position', '')
        }
    
    def _handle_404_link(self, parent_url, link_url, position_info, link_status):
        """å¤„ç†404é“¾æ¥"""
        filter_indicator = "ğŸ¯" if self.matches_path_filter(link_url) else "âšª"
        logger.info(f"    {filter_indicator} âŒ 404: {link_url}")
        logger.info(f"         ğŸ“ ä½ç½®: [{position_info['visual_position']}]")
        logger.info(f"         ğŸ¯ CSSé€‰æ‹©å™¨: {position_info.get('css_selector', 'æœªç”Ÿæˆ')}")
        
        if position_info['text']:
            logger.info(f"         ğŸ“ æ–‡æœ¬: {position_info['text']}")
        
        # æ˜¾ç¤ºclassä¿¡æ¯
        if position_info.get('classes_info'):
            logger.info(f"         ğŸ·ï¸  Classä¿¡æ¯:")
            for class_info in position_info['classes_info']:
                logger.info(f"             {class_info['level']}: <{class_info['tag']}> class=\"{class_info['classes']}\"")
        
        if position_info.get('element_id'):
            logger.info(f"         ğŸ†” å…ƒç´ ID: {position_info['element_id']}")
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        fix_suggestion = self.generate_fix_suggestion({
            'url': link_url,
            'visual_position': position_info.get('visual_position', '')
        })
        logger.info(f"         ğŸ’¡ ä¿®å¤å»ºè®®: {fix_suggestion}")
        
        # æ·»åŠ åˆ°404åˆ—è¡¨
        with self._lock:
            self.found_404s.append({
                'url': link_url,
                'found_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'status_code': 404,
                'parent_page': parent_url,
                'matches_filter': self.matches_path_filter(link_url),
                'position': position_info['position'],
                'link_text': position_info['text'],
                'element_type': position_info['element_type'],
                'classes_info': position_info.get('classes_info', []),
                'element_id': position_info.get('element_id', ''),
                'element_tag': position_info.get('element_tag', ''),
                'css_selector': position_info.get('css_selector', ''),
                'xpath': position_info.get('xpath', ''),
                'visual_position': position_info.get('visual_position', ''),
                'fix_suggestion': fix_suggestion
            })
    
    def _save_page_details(self, url, links, page_links_status):
        """ä¿å­˜é¡µé¢è¯¦æƒ…"""
        with self._lock:
            self.page_link_details.append({
                'page_url': url,
                'total_links': len(links),
                'links_status': page_links_status,
                'scan_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        
        # ç»Ÿè®¡404é“¾æ¥
        status_404_count = sum(1 for link_status in page_links_status 
                              if link_status['status_code'] == 404)
        
        if status_404_count > 0:
            logger.info(f"  ğŸ“Š å‘ç° {status_404_count} ä¸ª404é“¾æ¥")
    
    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡æ‘˜è¦"""
        total_404s = len(self.found_404s)
        filtered_404s = sum(1 for link in self.found_404s if link['matches_filter'])
        total_pages = len(self.visited_urls)
        total_links = len(self.all_links)
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¯ æ£€æµ‹å®Œæˆ - æœ€ç»ˆç»Ÿè®¡")
        logger.info("=" * 80)
        logger.info(f"ğŸ“„ å·²æ£€æµ‹é¡µé¢: {total_pages}")
        logger.info(f"ğŸ”— å‘ç°é“¾æ¥æ€»æ•°: {total_links}")
        logger.info(f"âŒ 404é“¾æ¥æ€»æ•°: {total_404s}")
        logger.info(f"ğŸ¯ ç¬¦åˆç­›é€‰æ¡ä»¶çš„404é“¾æ¥: {filtered_404s}")
        
        if total_404s > 0:
            # æŒ‰ä½ç½®åˆ†ç»„ç»Ÿè®¡
            position_stats = {}
            for link in self.found_404s:
                pos = link.get('visual_position', 'æœªçŸ¥åŒºåŸŸ')
                position_stats[pos] = position_stats.get(pos, 0) + 1
            
            logger.info("\nğŸ“Š 404é“¾æ¥ä½ç½®åˆ†å¸ƒ:")
            for position, count in sorted(position_stats.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"  {position}: {count} ä¸ª")
        
        logger.info("=" * 80)
    
    def crawl_for_404s(self, start_url=None):
        """çˆ¬å–åŸŸåä¸‹çš„æ‰€æœ‰é“¾æ¥å¹¶æ£€æµ‹404"""
        if start_url is None:
            start_url = self._get_start_url()
        
        self._print_crawl_info(start_url)
        
        # ğŸ”§ é’ˆå¯¹ /au è·¯å¾„çš„ç‰¹æ®Šå¤„ç†
        if self.path_filter and not start_url.endswith(self.path_filter.lstrip('/')):
            # å¦‚æœèµ·å§‹URLä¸åŒ…å«ç­›é€‰è·¯å¾„ï¼Œè‡ªåŠ¨æ·»åŠ 
            parsed = urlparse(start_url)
            start_url = f"{parsed.scheme}://{parsed.netloc}{self.path_filter}"
            logger.info(f"ğŸ¯ è‡ªåŠ¨è°ƒæ•´èµ·å§‹URLä¸º: {start_url}")
        
        url_queue = deque([start_url])
        pages_crawled = 0
        
        while url_queue and pages_crawled < self.max_pages:
            current_url = url_queue.popleft()
            
            if current_url in self.visited_urls:
                continue
            
            # ğŸ”§ å…³é”®ä¼˜åŒ–ï¼šåœ¨å¤„ç†å‰å°±æ£€æŸ¥è·¯å¾„ç­›é€‰
            if self.path_filter and not self.matches_path_filter(current_url):
                logger.info(f"â­ï¸  è·³è¿‡ä¸ç¬¦åˆç­›é€‰æ¡ä»¶çš„URL: {current_url}")
                continue
            
            self.visited_urls.add(current_url)
            pages_crawled += 1
            
            logger.info(f"ğŸ¯ å½“å‰é˜Ÿåˆ—é•¿åº¦: {len(url_queue)}, å·²è®¿é—®é¡µé¢: {len(self.visited_urls)}")
            logger.info(f"\nğŸ“– æ­£åœ¨çˆ¬å–ç¬¬ {pages_crawled}/{self.max_pages} é¡µ: {current_url}")
            
            status = self.check_url_status(current_url)
            
            if status == 404:
                self._handle_404_page(current_url)
            elif status == 'ERROR':
                logger.warning(f"âš ï¸  é¡µé¢çŠ¶æ€: è®¿é—®é”™è¯¯")
            elif status == 200:
                # æå–å¹¶æ£€æŸ¥é¡µé¢é“¾æ¥
                links = self.extract_and_check_links_from_page(current_url)
                self.all_links.update(links)
                
                # ğŸ”§ åªå°†ç¬¦åˆ /au è·¯å¾„ç­›é€‰æ¡ä»¶çš„é“¾æ¥åŠ å…¥é˜Ÿåˆ—
                new_links_added = 0
                filtered_links_added = 0
                
                for link in links:
                    if link not in self.visited_urls and link not in url_queue:
                        new_links_added += 1
                        # æ£€æŸ¥é“¾æ¥æ˜¯å¦ç¬¦åˆ /au è·¯å¾„ç­›é€‰æ¡ä»¶
                        if not self.path_filter or self.matches_path_filter(link):
                            url_queue.append(link)
                            filtered_links_added += 1
                
                logger.info(f"ğŸ”— å‘ç° {new_links_added} ä¸ªæ–°é“¾æ¥")
                logger.info(f"âœ… å…¶ä¸­ {filtered_links_added} ä¸ªç¬¦åˆ /au è·¯å¾„æ¡ä»¶ï¼Œå·²åŠ å…¥é˜Ÿåˆ—")
                
                if self.path_filter and filtered_links_added < new_links_added:
                    skipped = new_links_added - filtered_links_added
                    logger.info(f"â­ï¸  è·³è¿‡ {skipped} ä¸ªä¸åœ¨ /au è·¯å¾„ä¸‹çš„é“¾æ¥")
                
                if self.delay > 0:
                    time.sleep(self.delay)
            else:
                logger.info(f"  âš ï¸  é¡µé¢çŠ¶æ€: {status}")
    
    def _get_start_url(self):
        """è·å–èµ·å§‹URL"""
        return self.base_url
    
    def _print_crawl_info(self, start_url):
        """æ‰“å°çˆ¬å–ä¿¡æ¯"""
        logger.info(f"\nğŸš€ å¼€å§‹404é“¾æ¥æ£€æµ‹")
        logger.info(f"ğŸŒ ç›®æ ‡åŸŸå: {self.domain}")
        logger.info(f"ğŸ“„ æœ€å¤§é¡µé¢æ•°: {self.max_pages}")
        logger.info(f"ğŸ”„ å¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}")
        if self.path_filter:
            logger.info(f"ğŸ“ è·¯å¾„ç­›é€‰: {self.path_filter}")
            logger.info(f"ğŸ’¡ ç­–ç•¥: é¦–é¡µæ€»æ˜¯è¢«å¤„ç†ä»¥è·å–é“¾æ¥ï¼Œç„¶åå¯¹å‘ç°çš„é“¾æ¥åº”ç”¨ç­›é€‰")
        else:
            logger.info(f"ğŸ“ è·¯å¾„ç­›é€‰: æ— ï¼ˆæ£€æµ‹æ‰€æœ‰é¡µé¢ï¼‰")
        logger.info(f"â±ï¸  è¯·æ±‚å»¶è¿Ÿ: {self.delay}ç§’")
        logger.info(f"ğŸ¯ èµ·å§‹URL: {start_url}")
        
        # æµ‹è¯•èµ·å§‹URLçš„å¯è®¿é—®æ€§
        logger.info(f"ğŸ” æµ‹è¯•èµ·å§‹URLå¯è®¿é—®æ€§...")
        test_status = self.check_url_status(start_url)
        logger.info(f"ğŸ“Š èµ·å§‹URLçŠ¶æ€: {test_status}")
        
        if test_status == 'ERROR':
            logger.warning(f"âš ï¸  èµ·å§‹URLæ— æ³•è®¿é—®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–åŸŸåæ˜¯å¦æ­£ç¡®")
        
        logger.info("=" * 80)
    
    def _handle_404_page(self, url):
        """å¤„ç†404é¡µé¢"""
        logger.info(f"  âŒ é¡µé¢æœ¬èº«å°±æ˜¯404: {url}")
        with self._lock:
            self.found_404s.append({
                'url': url,
                'found_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'status_code': 404,
                'parent_page': 'N/A',
                'matches_filter': self.matches_path_filter(url),
                'position': 'é¡µé¢æœ¬èº«',
                'link_text': '',
                'element_type': 'page',
                'classes_info': [],
                'element_id': '',
                'element_tag': '',
                'css_selector': '',
                'xpath': '',
                'visual_position': 'é¡µé¢æœ¬èº«',
                'fix_suggestion': 'æ£€æŸ¥é¡µé¢æ˜¯å¦å·²åˆ é™¤æˆ–URLæ˜¯å¦æ­£ç¡®'
            })
    
    def save_results_to_excel(self):
        """ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶"""
        try:
            # åˆ›å»ºå·¥ä½œç°¿
            wb = Workbook()
            
            # 404é“¾æ¥æ±‡æ€»è¡¨
            ws_summary = wb.active
            ws_summary.title = "404é“¾æ¥æ±‡æ€»"
            
            # è®¾ç½®è¡¨å¤´
            headers = [
                '404é“¾æ¥', 'å‘ç°æ—¶é—´', 'çŠ¶æ€ç ', 'æ¥æºé¡µé¢', 'åŒ¹é…ç­›é€‰æ¡ä»¶', 
                'å¯è§†åŒ–ä½ç½®', 'CSSé€‰æ‹©å™¨', 'XPathè·¯å¾„', 'é“¾æ¥æ–‡æœ¬', 'å…ƒç´ ç±»å‹', 
                'å…ƒç´ æ ‡ç­¾', 'å…ƒç´ ID', 'Classä¿¡æ¯', 'ä¿®å¤å»ºè®®'
            ]
            
            # è®¾ç½®è¡¨å¤´æ ·å¼
            header_font = Font(bold=True, color="FFFFFF")
            header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
            
            for col, header in enumerate(headers, 1):
                cell = ws_summary.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal="center")
            
            # å¡«å……404é“¾æ¥æ•°æ®
            for row, link_404 in enumerate(self.found_404s, 2):
                ws_summary.cell(row=row, column=1, value=link_404['url'])
                ws_summary.cell(row=row, column=2, value=link_404['found_time'])
                ws_summary.cell(row=row, column=3, value=link_404['status_code'])
                ws_summary.cell(row=row, column=4, value=link_404['parent_page'])
                ws_summary.cell(row=row, column=5, value='æ˜¯' if link_404['matches_filter'] else 'å¦')
                ws_summary.cell(row=row, column=6, value=link_404.get('visual_position', ''))
                ws_summary.cell(row=row, column=7, value=link_404.get('css_selector', ''))
                ws_summary.cell(row=row, column=8, value=link_404.get('xpath', ''))
                ws_summary.cell(row=row, column=9, value=link_404['link_text'])
                ws_summary.cell(row=row, column=10, value=link_404['element_type'])
                ws_summary.cell(row=row, column=11, value=link_404['element_tag'])
                ws_summary.cell(row=row, column=12, value=link_404['element_id'])
                
                # æ ¼å¼åŒ–classä¿¡æ¯
                class_info_str = ''
                for class_info in link_404.get('classes_info', []):
                    class_info_str += f"{class_info['level']}: <{class_info['tag']}> class=\"{class_info['classes']}\"\n"
                ws_summary.cell(row=row, column=13, value=class_info_str.strip())
                
                ws_summary.cell(row=row, column=14, value=link_404.get('fix_suggestion', ''))
            
            # è®¾ç½®åˆ—å®½
            column_widths = [50, 20, 10, 50, 15, 25, 40, 40, 30, 15, 15, 20, 60, 40]
            for col, width in enumerate(column_widths, 1):
                ws_summary.column_dimensions[ws_summary.cell(row=1, column=col).column_letter].width = width
            
            # é¡µé¢é“¾æ¥è¯¦æƒ…å·¥ä½œè¡¨
            ws_details = wb.create_sheet("é¡µé¢é“¾æ¥è¯¦æƒ…")
            detail_headers = [
                'é¡µé¢URL', 'é“¾æ¥URL', 'çŠ¶æ€ç ', 'æ£€æŸ¥æ—¶é—´', 'åŒ¹é…ç­›é€‰æ¡ä»¶', 
                'å¯è§†åŒ–ä½ç½®', 'CSSé€‰æ‹©å™¨', 'XPathè·¯å¾„', 'é“¾æ¥æ–‡æœ¬', 'å…ƒç´ ç±»å‹', 
                'å…ƒç´ æ ‡ç­¾', 'å…ƒç´ ID', 'Classä¿¡æ¯'
            ]
            
            # è®¾ç½®è¯¦æƒ…è¡¨å¤´æ ·å¼
            for col, header in enumerate(detail_headers, 1):
                cell = ws_details.cell(row=1, column=col, value=header)
                cell.font = header_font
                cell.fill = header_fill
                cell.alignment = Alignment(horizontal="center")
            
            detail_row = 2
            for page_detail in self.page_link_details:
                for link_status in page_detail['links_status']:
                    ws_details.cell(row=detail_row, column=1, value=page_detail['page_url'])
                    ws_details.cell(row=detail_row, column=2, value=link_status['link_url'])
                    ws_details.cell(row=detail_row, column=3, value=link_status['status_code'])
                    ws_details.cell(row=detail_row, column=4, value=link_status['check_time'])
                    ws_details.cell(row=detail_row, column=5, value='æ˜¯' if link_status['matches_filter'] else 'å¦')
                    ws_details.cell(row=detail_row, column=6, value=link_status.get('visual_position', ''))
                    ws_details.cell(row=detail_row, column=7, value=link_status.get('css_selector', ''))
                    ws_details.cell(row=detail_row, column=8, value=link_status.get('xpath', ''))
                    ws_details.cell(row=detail_row, column=9, value=link_status['link_text'])
                    ws_details.cell(row=detail_row, column=10, value=link_status['element_type'])
                    ws_details.cell(row=detail_row, column=11, value=link_status.get('element_tag', ''))
                    ws_details.cell(row=detail_row, column=12, value=link_status.get('element_id', ''))
                    
                    # æ ¼å¼åŒ–classä¿¡æ¯
                    class_info_str = ''
                    for class_info in link_status.get('classes_info', []):
                        class_info_str += f"{class_info['level']}: <{class_info['tag']}> class=\"{class_info['classes']}\"\n"
                    ws_details.cell(row=detail_row, column=13, value=class_info_str.strip())
                    
                    detail_row += 1
            
            # è®¾ç½®è¯¦æƒ…è¡¨åˆ—å®½
            detail_column_widths = [50, 50, 10, 20, 15, 25, 40, 40, 30, 15, 15, 20, 60]
            for col, width in enumerate(detail_column_widths, 1):
                ws_details.column_dimensions[ws_details.cell(row=1, column=col).column_letter].width = width
            
            # ç»Ÿè®¡ä¿¡æ¯å·¥ä½œè¡¨
            ws_stats = wb.create_sheet("ç»Ÿè®¡ä¿¡æ¯")
            stats_data = [
                ['æ£€æµ‹åŸŸå', self.domain],
                ['æ£€æµ‹æ—¶é—´', datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
                ['å·²è®¿é—®é¡µé¢æ•°', len(self.visited_urls)],
                ['å‘ç°é“¾æ¥æ€»æ•°', len(self.all_links)],
                ['404é“¾æ¥æ•°é‡', len(self.found_404s)],
                ['è·¯å¾„ç­›é€‰æ¡ä»¶', str(self.path_filter) if self.path_filter else 'æ— '],
            ]
            
            if self.path_filter:
                filtered_404s = [link for link in self.found_404s if link['matches_filter']]
                stats_data.append(['ç¬¦åˆç­›é€‰æ¡ä»¶çš„404é“¾æ¥', len(filtered_404s)])
            
            for row, (key, value) in enumerate(stats_data, 1):
                ws_stats.cell(row=row, column=1, value=key)
                ws_stats.cell(row=row, column=2, value=value)
            
            ws_stats.column_dimensions['A'].width = 25
            ws_stats.column_dimensions['B'].width = 50
            
            # ä¿å­˜Excelæ–‡ä»¶
            domain_safe = self.domain.replace('.', '_').replace('://', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"404_links_{domain_safe}_{timestamp}.xlsx"
            
            wb.save(filename)
            logger.info(f"\nğŸ“Š ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
            
            return filename
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None
    
    def generate_html_report(self):
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        try:
            html_content = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <title>404é“¾æ¥æ£€æµ‹æŠ¥å‘Š - {self.domain}</title>
                <meta charset="UTF-8">
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                    .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                    h1 {{ color: #333; text-align: center; border-bottom: 3px solid #007bff; padding-bottom: 10px; }}
                    .summary {{ background: #e9ecef; padding: 15px; border-radius: 5px; margin: 20px 0; }}
                    .link-item {{ border: 1px solid #ddd; margin: 15px 0; padding: 20px; border-radius: 8px; background: #fff; }}
                    .link-item:hover {{ box-shadow: 0 4px 8px rgba(0,0,0,0.1); }}
                    .url {{ color: #d32f2f; font-weight: bold; font-size: 16px; word-break: break-all; }}
                    .position {{ color: #1976d2; background: #e3f2fd; padding: 5px 10px; border-radius: 3px; display: inline-block; margin: 5px 0; }}
                    .selector {{ background: #f8f9fa; padding: 10px; font-family: 'Courier New', monospace; border-left: 4px solid #007bff; margin: 10px 0; word-break: break-all; }}
                    .suggestion {{ background: #fff3cd; border: 1px solid #ffeaa7; padding: 10px; border-radius: 5px; margin: 10px 0; }}
                    .meta {{ color: #666; font-size: 14px; }}
                    .tag {{ background: #6c757d; color: white; padding: 2px 6px; border-radius: 3px; font-size: 12px; margin: 2px; }}
                    .stats {{ display: flex; justify-content: space-around; margin: 20px 0; }}
                    .stat-item {{ text-align: center; padding: 15px; background: #f8f9fa; border-radius: 5px; }}
                    .stat-number {{ font-size: 24px; font-weight: bold; color: #007bff; }}
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>ğŸ” 404é“¾æ¥æ£€æµ‹æŠ¥å‘Š</h1>
                    
                    <div class="summary">
                        <p><strong>ğŸŒ æ£€æµ‹åŸŸå:</strong> {self.domain}</p>
                        <p><strong>â° æ£€æµ‹æ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                    </div>
                    
                    <div class="stats">
                        <div class="stat-item">
                            <div class="stat-number">{len(self.visited_urls)}</div>
                            <div>å·²æ£€æµ‹é¡µé¢</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">{len(self.all_links)}</div>
                            <div>å‘ç°é“¾æ¥æ€»æ•°</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">{len(self.found_404s)}</div>
                            <div>404é“¾æ¥æ•°é‡</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">{sum(1 for link in self.found_404s if link['matches_filter'])}</div>
                            <div>ç¬¦åˆç­›é€‰æ¡ä»¶</div>
                        </div>
                    </div>
            """
            
            for i, link_404 in enumerate(self.found_404s, 1):
                html_content += f"""
                <div class="link-item">
                    <h3>#{i} <span class="url">{link_404['url']}</span></h3>
                    <div class="meta">
                        <p><strong>ğŸ“„ æ¥æºé¡µé¢:</strong> {link_404['parent_page']}</p>
                        <p><strong>â° å‘ç°æ—¶é—´:</strong> {link_404['found_time']}</p>
                        <p><strong>ğŸ“ ä½ç½®:</strong> <span class="position">{link_404.get('visual_position', 'æœªçŸ¥')}</span></p>
                        {f'<p><strong>ğŸ“ é“¾æ¥æ–‡æœ¬:</strong> {link_404["link_text"]}</p>' if link_404.get('link_text') else ''}
                        <p><strong>ğŸ·ï¸ å…ƒç´ ç±»å‹:</strong> <span class="tag">{link_404['element_type']}</span> 
                           <span class="tag">{link_404['element_tag']}</span>
                           {f'<span class="tag">ID: {link_404["element_id"]}</span>' if link_404.get('element_id') else ''}</p>
                    </div>
                    
                    <div class="selector">
                        <strong>ğŸ¯ CSSé€‰æ‹©å™¨:</strong><br>
                        <code>{link_404.get('css_selector', 'æœªç”Ÿæˆ')}</code>
                    </div>
                    
                    <div class="selector">
                        <strong>ğŸ—ºï¸ XPathè·¯å¾„:</strong><br>
                        <code>{link_404.get('xpath', 'æœªç”Ÿæˆ')}</code>
                    </div>
                    
                    <div class="suggestion">
                        <strong>ğŸ’¡ ä¿®å¤å»ºè®®:</strong> {link_404.get('fix_suggestion', 'æ‰‹åŠ¨æ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§')}
                    </div>
                </div>
                """
            
            html_content += """
                </div>
            </body>
            </html>
            """
            
            # ä¿å­˜HTMLæ–‡ä»¶
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"404_report_{self.domain.replace('.', '_')}_{timestamp}.html"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"ç”ŸæˆHTMLæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            return None
    
    def save_json_report(self):
        """ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
        try:
            report_data = {
                'scan_info': {
                    'domain': self.domain,
                    'scan_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'max_pages': self.max_pages,
                    'path_filter': self.path_filter,
                    'total_pages_scanned': len(self.visited_urls),
                    'total_links_found': len(self.all_links),
                    'total_404s_found': len(self.found_404s)
                },
                'found_404s': self.found_404s,
                'page_details': self.page_link_details
            }
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"404_data_{self.domain.replace('.', '_')}_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“‹ JSONæ•°æ®å·²ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if hasattr(self, 'session'):
                self.session.close()
            logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
    
    def _generate_css_selector(self, element):
        """ç”ŸæˆCSSé€‰æ‹©å™¨"""
        try:
            selectors = []
            current = element
            
            while current and current.name and len(selectors) < 5:
                selector_part = current.name
                
                # æ·»åŠ ID
                if current.get('id'):
                    selector_part += f"#{current['id']}"
                    selectors.append(selector_part)
                    break  # IDæ˜¯å”¯ä¸€çš„ï¼Œå¯ä»¥åœæ­¢
                
                # æ·»åŠ class
                classes = current.get('class', [])
                if classes:
                    # åªä½¿ç”¨å‰ä¸¤ä¸ªclassä»¥é¿å…é€‰æ‹©å™¨è¿‡é•¿
                    class_str = '.'.join(classes[:2])
                    selector_part += f".{class_str}"
                
                selectors.append(selector_part)
                current = current.parent
            
            return ' > '.join(reversed(selectors))
            
        except Exception:
            return ''
    
    def _generate_xpath(self, element):
        """ç”ŸæˆXPathè·¯å¾„"""
        try:
            xpath_parts = []
            current = element
            
            while current and current.name and len(xpath_parts) < 5:
                tag = current.name
                
                # å¦‚æœæœ‰IDï¼Œä½¿ç”¨IDå®šä½
                if current.get('id'):
                    xpath_parts.append(f"//{tag}[@id='{current['id']}']")
                    break
                
                # è®¡ç®—åŒçº§å…ƒç´ ä¸­çš„ä½ç½®
                siblings = [s for s in current.parent.children if hasattr(s, 'name') and s.name == tag] if current.parent else []
                if len(siblings) > 1:
                    index = siblings.index(current) + 1
                    xpath_parts.append(f"{tag}[{index}]")
                else:
                    xpath_parts.append(tag)
                
                current = current.parent
            
            if xpath_parts:
                return '/' + '/'.join(reversed(xpath_parts))
            return ''
            
        except Exception:
            return ''
    
    def _determine_visual_position(self, positions, element_classes):
        """ç¡®å®šå…ƒç´ çš„å¯è§†åŒ–ä½ç½®"""
        # åŸºäºä½ç½®ä¿¡æ¯ç¡®å®šå¯è§†åŒ–åŒºåŸŸ
        position_str = ' '.join(positions).lower()
        class_str = ' '.join(element_classes).lower() if element_classes else ''
        
        combined = f"{position_str} {class_str}"
        
        if any(keyword in combined for keyword in ['header', 'top', 'navbar', 'nav-bar']):
            return 'é¡µé¢å¤´éƒ¨'
        elif any(keyword in combined for keyword in ['footer', 'bottom', 'foot']):
            return 'é¡µé¢åº•éƒ¨'
        elif any(keyword in combined for keyword in ['sidebar', 'side-bar', 'aside']):
            return 'ä¾§è¾¹æ '
        elif any(keyword in combined for keyword in ['menu', 'nav', 'navigation']):
            return 'å¯¼èˆªèœå•'
        elif any(keyword in combined for keyword in ['breadcrumb']):
            return 'é¢åŒ…å±‘å¯¼èˆª'
        elif any(keyword in combined for keyword in ['pagination', 'pager']):
            return 'åˆ†é¡µåŒºåŸŸ'
        elif any(keyword in combined for keyword in ['search']):
            return 'æœç´¢åŒºåŸŸ'
        elif any(keyword in combined for keyword in ['social', 'share']):
            return 'ç¤¾äº¤åˆ†äº«åŒºåŸŸ'
        elif any(keyword in combined for keyword in ['product', 'item', 'card']):
            return 'äº§å“/å†…å®¹å¡ç‰‡'
        elif any(keyword in combined for keyword in ['category', 'section']):
            return 'åˆ†ç±»/æ ç›®åŒºåŸŸ'
        elif any(keyword in combined for keyword in ['banner', 'hero', 'slider']):
            return 'æ¨ªå¹…/è½®æ’­åŒºåŸŸ'
        elif any(keyword in combined for keyword in ['form', 'contact']):
            return 'è¡¨å•åŒºåŸŸ'
        elif any(keyword in combined for keyword in ['main', 'content', 'body']):
            return 'ä¸»è¦å†…å®¹åŒºåŸŸ'
        else:
            return 'é¡µé¢ä¸»ä½“'
    
    def generate_fix_suggestion(self, link_info):
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        url = link_info.get('url', '')
        position = link_info.get('visual_position', '')
        
        suggestions = []
        
        # åŸºäºURLæ¨¡å¼çš„å»ºè®®
        if '/product' in url.lower():
            suggestions.append('æ£€æŸ¥äº§å“æ˜¯å¦å·²ä¸‹æ¶æˆ–IDæ˜¯å¦æ­£ç¡®')
        elif '/category' in url.lower():
            suggestions.append('æ£€æŸ¥åˆ†ç±»æ˜¯å¦å·²åˆ é™¤æˆ–é‡å‘½å')
        elif '/blog' in url.lower() or '/post' in url.lower():
            suggestions.append('æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²åˆ é™¤æˆ–URLç»“æ„æ˜¯å¦å˜æ›´')
        elif '/user' in url.lower() or '/profile' in url.lower():
            suggestions.append('æ£€æŸ¥ç”¨æˆ·è´¦æˆ·æ˜¯å¦å­˜åœ¨æˆ–æƒé™è®¾ç½®')
        
        # åŸºäºä½ç½®çš„å»ºè®®
        if 'å¯¼èˆª' in position:
            suggestions.append('æ›´æ–°å¯¼èˆªèœå•é…ç½®')
        elif 'åº•éƒ¨' in position:
            suggestions.append('æ£€æŸ¥é¡µè„šé“¾æ¥é…ç½®')
        elif 'ä¾§è¾¹æ ' in position:
            suggestions.append('æ›´æ–°ä¾§è¾¹æ ç»„ä»¶')
        
        # é€šç”¨å»ºè®®
        suggestions.extend([
            'æ£€æŸ¥ç›®æ ‡é¡µé¢æ˜¯å¦å­˜åœ¨',
            'éªŒè¯URLæ‹¼å†™æ˜¯å¦æ­£ç¡®',
            'ç¡®è®¤é“¾æ¥ç›®æ ‡æ˜¯å¦å·²è¿ç§»'
        ])
        
        return '; '.join(suggestions[:3])  # è¿”å›å‰3ä¸ªå»ºè®®

def get_user_config():
    """è·å–ç”¨æˆ·é…ç½®"""
    print("\nğŸ”§ 404é“¾æ¥æ£€æµ‹å·¥å…·é…ç½®")
    print("=" * 50)
    
    # è·å–ç›®æ ‡åŸŸå
    while True:
        domain = input("\nğŸŒ è¯·è¾“å…¥ç›®æ ‡åŸŸå (ä¾‹å¦‚: www.anker.com): ").strip()
        if domain:
            break
        print("âŒ åŸŸåä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
    
    # è·å–è·¯å¾„ç­›é€‰
    print("\nğŸ“ é€‰æ‹©æ£€æµ‹èŒƒå›´:")
    print("1. æ£€æµ‹æ‰€æœ‰é¡µé¢ (æ— ç­›é€‰)")
    print("2. åªæ£€æµ‹æ¾³æ´²ç«™ç‚¹ (/au)")
    print("3. åªæ£€æµ‹äº§å“é¡µé¢ (/products)")
    print("4. åªæ£€æµ‹æ”¯æŒé¡µé¢ (/support)")
    print("5. åªæ£€æµ‹åšå®¢é¡µé¢ (/blog)")
    print("6. è‡ªå®šä¹‰è·¯å¾„ç­›é€‰")
    print("7. å¤šè·¯å¾„ç­›é€‰")
    
    while True:
        try:
            filter_choice = int(input("\nè¯·é€‰æ‹© (1-7): ").strip())
            if 1 <= filter_choice <= 7:
                break
            print("âŒ è¯·è¾“å…¥1-7ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    path_filter = None
    if filter_choice == 2:
        path_filter = '/au'
        print("âœ… å·²é€‰æ‹©æ¾³æ´²ç«™ç‚¹ï¼Œå°†åªæ£€æµ‹ /au è·¯å¾„ä¸‹çš„é¡µé¢")
    elif filter_choice == 3:
        path_filter = '/products'
    elif filter_choice == 4:
        path_filter = '/support'
    elif filter_choice == 5:
        path_filter = '/blog'
    elif filter_choice == 6:
        custom_path = input("\nè¯·è¾“å…¥è‡ªå®šä¹‰è·¯å¾„ (ä¾‹å¦‚: /au, /category): ").strip()
        if custom_path:
            if not custom_path.startswith('/'):
                custom_path = '/' + custom_path
            path_filter = custom_path
    elif filter_choice == 7:
        print("\nè¯·è¾“å…¥å¤šä¸ªè·¯å¾„ï¼Œç”¨é€—å·åˆ†éš” (ä¾‹å¦‚: /au,/products,/support):")
        multi_paths = input().strip()
        if multi_paths:
            paths = [p.strip() for p in multi_paths.split(',')]
            paths = [p if p.startswith('/') else '/' + p for p in paths if p]
            if paths:
                path_filter = paths
    
    # è·å–æœ€å¤§æ£€æµ‹é¡µé¢æ•°
    while True:
        try:
            max_pages = int(input("\nğŸ“„ æœ€å¤§æ£€æµ‹é¡µé¢æ•° (å»ºè®®10-100ï¼Œé»˜è®¤50): ").strip() or "50")
            if max_pages > 0:
                break
            print("âŒ é¡µé¢æ•°å¿…é¡»å¤§äº0")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    # è·å–å¹¶å‘çº¿ç¨‹æ•°
    while True:
        try:
            max_workers = int(input("\nğŸ”„ å¹¶å‘çº¿ç¨‹æ•° (å»ºè®®3-10ï¼Œé»˜è®¤5): ").strip() or "5")
            if 1 <= max_workers <= 20:
                break
            print("âŒ çº¿ç¨‹æ•°å»ºè®®åœ¨1-20ä¹‹é—´")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    # è·å–è¯·æ±‚å»¶è¿Ÿ
    while True:
        try:
            delay = float(input("\nâ±ï¸ è¯·æ±‚å»¶è¿Ÿç§’æ•° (å»ºè®®0.5-2ï¼Œé»˜è®¤1): ").strip() or "1")
            if delay >= 0:
                break
            print("âŒ å»¶è¿Ÿæ—¶é—´ä¸èƒ½ä¸ºè´Ÿæ•°")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    # ç¡®è®¤é…ç½®
    print("\nğŸ“‹ é…ç½®ç¡®è®¤:")
    print(f"ğŸŒ ç›®æ ‡åŸŸå: {domain}")
    if isinstance(path_filter, list):
        print(f"ğŸ“ æ£€æµ‹è·¯å¾„: {', '.join(path_filter)}")
    else:
        print(f"ğŸ“ æ£€æµ‹è·¯å¾„: {path_filter if path_filter else 'æ‰€æœ‰é¡µé¢'}")
    print(f"ğŸ“„ æœ€å¤§é¡µé¢æ•°: {max_pages}")
    print(f"ğŸ”„ å¹¶å‘çº¿ç¨‹æ•°: {max_workers}")
    print(f"â±ï¸ è¯·æ±‚å»¶è¿Ÿ: {delay}ç§’")
    
    confirm = input("\nâœ… ç¡®è®¤å¼€å§‹æ£€æµ‹ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").strip().lower()
    if confirm in ['n', 'no']:
        print("âŒ ç”¨æˆ·å–æ¶ˆæ£€æµ‹")
        return None
    
    return {
        'domain': domain,
        'path_filter': path_filter,
        'max_pages': max_pages,
        'max_workers': max_workers,
        'delay': delay
    }

def main():
    """ä¸»å‡½æ•°"""
    try:
        # è·å–ç”¨æˆ·é…ç½®
        config = get_user_config()
        if not config:
            return
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶å¼€å§‹æ£€æµ‹
        with Link404Crawler(
            domain=config['domain'],
            max_pages=config['max_pages'],
            delay=config['delay'],
            path_filter=config['path_filter'],
            max_workers=config['max_workers']
        ) as crawler:
            
            # å¼€å§‹çˆ¬å–
            crawler.crawl_for_404s()
            
            # æ‰“å°æœ€ç»ˆæ‘˜è¦
            crawler.print_final_summary()
            
            # ä¿®å¤ï¼šæ— è®ºæ˜¯å¦å‘ç°404éƒ½ä¿å­˜æŠ¥å‘Š
            print("\nğŸ’¾ æ­£åœ¨ä¿å­˜æ£€æµ‹ç»“æœ...")
            
            # ä¿å­˜ExcelæŠ¥å‘Š
            excel_file = crawler.save_results_to_excel()
            
            # ä¿å­˜HTMLæŠ¥å‘Š
            html_file = crawler.generate_html_report()
            
            # ä¿å­˜JSONæ•°æ®
            json_file = crawler.save_json_report()
            
            print("\nğŸ‰ æ£€æµ‹å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
            if excel_file:
                print(f"ğŸ“Š ExcelæŠ¥å‘Š: {excel_file}")
            if html_file:
                print(f"ğŸ“„ HTMLæŠ¥å‘Š: {html_file}")
            if json_file:
                print(f"ğŸ“‹ JSONæ•°æ®: {json_file}")
                
            if not crawler.found_404s:
                print("\nğŸ‰ å¤ªæ£’äº†ï¼æ²¡æœ‰å‘ç°404é“¾æ¥ï¼")
                
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ£€æµ‹")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()